{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy, os\n",
    "from arcpy.sa import *\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_hydro = [ \"RC1\", \"RC2\", \"RC10\", \"RC1_O30\", \"RC1_O50\", \"RC1_C30\", \"RC1_C50\", \"RC1_P30\", \"RC1_P50\", \"RC2_O30\", \"RC2_O50\", \"RC2_C30\", \"RC2_C50\", \"RC2_P30\", \"RC2_P50\"]\n",
    "vars_chg = [\"RC1_O3rc\", \"RC1_O5rc\", \"RC1_C3rc\", \"RC1_C5rc\", \"RC1_P3rc\", \"RC1_P5rc\", \"RC2_O3rc\", \"RC2_O5rc\", \"RC2_C3rc\", \"RC2_C5rc\", \"RC2_P3rc\", \"RC2_P5rc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vars = vars_hydro + vars_chg\n",
    "all_vars.append(\"area\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open pickled dictionaries\n",
    "with open('grand_stats_1', 'rb') as infile:\n",
    "    grand_dict1 = pickle.load(infile)\n",
    "with open('grand_stats_2', 'rb') as infile:\n",
    "    grand_dict2 = pickle.load(infile)\n",
    "with open('grand_stats_3', 'rb') as infile:\n",
    "    grand_dict3 = pickle.load(infile)\n",
    "with open('grand_stats_fixed', 'rb') as infile:\n",
    "    grand_dict3 = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine batched dictionaries\n",
    "grand_dict = {**grand_dict1, **grand_dict2, **grand_dict3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overwrite updated values -- these are catchments which needed revision after the first round\n",
    "\n",
    "grand_dict.update(grand_dict_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create a dataframe from the dictionary\n",
    "\n",
    "def make_table(dict_now, lyr, vars_now, type=\"grand\"):\n",
    "    table = pd.DataFrame.from_dict(dict_now, orient='index',  columns=all_vars)\n",
    "    table = table.reset_index()\n",
    "    table = table.rename(columns={'index':\"dam_ID\"})\n",
    "    \n",
    "    # get areas from dam point attribute tables\n",
    "    areas = {}\n",
    "    with arcpy.da.SearchCursor(lyr,vars_now) as cursor:\n",
    "        for row in cursor:\n",
    "            if type == \"fhred\":\n",
    "                areas[str(row[0])[:-2]] = row[1]\n",
    "            else:\n",
    "                areas[str(row[0])] = row[1]\n",
    "    table2 = pd.DataFrame.from_dict(areas, orient='index',  columns=['catch_skm'])\n",
    "    table2 = table2.reset_index()\n",
    "    table2 = table2.rename(columns={'index':\"dam_ID\"})\n",
    "    \n",
    "    # combine into one dataframe, compared calculated catchment area to given area\n",
    "    table_results = pd.merge(table, table2, how=\"left\", on=\"dam_ID\")\n",
    "    table_results['check_area'] = table_results['area'] - table_results['catch_skm']\n",
    "    table_results['area_pct'] = table_results['check_area']/table_results['catch_skm']\n",
    "    res = table['dam_ID'].tolist()\n",
    "    res2 = table2['dam_ID'].tolist()\n",
    "    for i in res2:\n",
    "        if i not in res:\n",
    "            print (i)\n",
    "    return table_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run function to create GRanD dataframe\n",
    "\n",
    "grand_results = make_table(grand_dict, \"GRanD_v1_3_selection\", ['GRAND_ID', 'CATCH_SKM'])\n",
    "print(len(grand_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dataframe\n",
    "grand_results.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all catchments which have a difference in catchment area of at least 2% and 20km2\n",
    "# at the end there was only one of these remaining, which I checked manually and the catchment all falls within the same level 7 basin so it doesn't change the result\n",
    "\n",
    "grand_problems = grand_results[((grand_results['area_pct'] > 0.02) | (grand_results['area_pct'] < -0.02)) & ((grand_results['check_area']> 20) | (grand_results['check_area'] <-20))]\n",
    "grand_problems.sort_values(by='check_area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find any results which could not be calculated (script to create dictionaries gave a value of -9999 in this case)\n",
    "grand_results[grand_results.isin([-9999]).any(axis=1)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open fhred dictionaries and combine\n",
    "\n",
    "with open('fhred_stats', 'rb') as infile:\n",
    "    fhred0 = pickle.load(infile)\n",
    "    print(len(fhred0))\n",
    "with open('fhred_stats_1', 'rb') as infile:\n",
    "    fhred1 = pickle.load(infile)\n",
    "    print(len(fhred1))\n",
    "with open('fhred_stats_2', 'rb') as infile:\n",
    "    fhred2 = pickle.load(infile)\n",
    "    print(len(fhred2))\n",
    "with open('fhred_stats_3', 'rb') as infile:\n",
    "    fhred3 = pickle.load(infile)\n",
    "    print(len(fhred3))\n",
    "fhred_dict = {**fhred0, **fhred1, **fhred2, **fhred3}\n",
    "print(len(fhred_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overwrite updated values\n",
    "with open('fhred_stats_redo_all_final', 'rb') as infile:\n",
    "    fhred_redo = pickle.load(infile)\n",
    "    print(len(fhred_redo))\n",
    "fhred_dict.update(fhred_redo)\n",
    "print(len(fhred_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create table of fhred results\n",
    "fhred_results = make_table(fhred_dict, \"FHReD2015_withGOID\", [\"DAM_ID\", \"UPLAND_SKM\"], \"fhred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dataframe\n",
    "fhred_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check results make sense\n",
    "fhred_results.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find any results which could not be calculated (script to create dictionaries gave a value of -9999 in this case)\n",
    "fhred_nd = fhred_results[fhred_results.isin([-9999]).any(axis=1)].sort_values(by='check_area')['dam_ID'].to_list()\n",
    "print(fhred_nd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for errors by comparing calculated vs. given catchment areas\n",
    "# I checked the remaining areas and they are small enough/within a level 7 basin to not make a difference in the risk calculations\n",
    "fhred_problems = fhred_results[((fhred_results['area_pct'] > 0.05) | (fhred_results['area_pct'] < -0.05)) & ((fhred_results['check_area']> 20) | (fhred_results['check_area'] <-20))]\n",
    "fhred_problems.sort_values(by='dam_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of all dams which need to be redone\n",
    "for i in fhred_problems['dam_ID'].to_list():\n",
    "    if i not in fhred_nd:\n",
    "        fhred_nd.append(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "Python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
